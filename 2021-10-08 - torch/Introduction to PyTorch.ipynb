{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение NumPy и PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принципы работы в PyTorch немногим отличаются от операций с массивами в NumPy. Рассмотрим следующую функцию ошибки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L=\\| y - X\\cdot \\Theta\\|^2_2+\\lambda \\|\\Theta\\|_1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную функцию можно последовательно реализовать средствами NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randn(2, 1)\n",
    "X = np.random.randn(2, 5)\n",
    "theta = np.random.randn(5, 1)\n",
    "lmbda = 0.1\n",
    "\n",
    "pred = X.dot(theta)\n",
    "pred_loss = np.sum((y - pred) ** 2)\n",
    "reg_loss = lmbda * np.sum(np.abs(theta))\n",
    "loss = pred_loss + reg_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На PyTorch реализация вычисления функции $L$ выглядит аналогично, однако вместо массивов NumPy используются специальные объекты &ndash; `torch.Tensor`. Создать тензор в PyTorch можно как из простого массива, так и из массива Numpy, при этом указав тип данных, содержащихся в тензоре (по умолчанию float32).\n",
    "\n",
    "Подробнее о типах данных в PyTorch: https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor(y)\n",
    "X = torch.Tensor(X)\n",
    "theta = torch.Tensor(theta).requires_grad_(True) # для theta понадобится вычислить градиент\n",
    "\n",
    "pred = X.matmul(theta)\n",
    "pred_loss = torch.sum((y - pred) ** 2)\n",
    "reg_loss = lmbda * torch.sum(torch.abs(theta))\n",
    "loss = pred_loss + reg_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.Tensor([1, 2, 3]).type())\n",
    "print(torch.Tensor(np.array([1, 2, 3])).type())\n",
    "print(torch.LongTensor([1, 2, 3]).type())\n",
    "print(torch.zeros([2, 2, 3], dtype=torch.int64).type())\n",
    "\n",
    "print(torch.Tensor([[[1, 2, 3], [4, 5, 6]],[[7, 8, 9],[10, 11, 12]]]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей градиентными методами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение моделей в PyTorch производится в цикле. Ниже приведена реализация метода градиентного спуска. Чтобы обучать модели градиентными методами необходимо помнить о том, что на каждой итерации необходимо \"обнулять градиенты\" (в примере ниже `theta.grad.zero_()`), поскольку по умолчанию PyTorch накапливает градиент, а не вычисляет его заново для текущих значений параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    pred = X.matmul(theta)\n",
    "    pred_loss = torch.sum((y - pred) ** 2)\n",
    "    reg_loss = lmbda * torch.sum(torch.abs(theta))\n",
    "    loss = pred_loss + reg_loss\n",
    "\n",
    "    loss.backward() # вычисляет градиент функции по указанным переменным при их текущих значениях\n",
    "    theta.data.add_(-0.1*theta.grad.data)\n",
    "    theta.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижнее подчеркивание у функций перезаписывает значения переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([50, 45, 8])\n",
    "b = 5\n",
    "\n",
    "print(a.add(b))\n",
    "print(a)\n",
    "print(a.add_(b))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование архитектуры нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бинарная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае бинарной классификации в качестве функции потерь используется, как правило, бинарная кросс-энтропия. Она реализована в PyTorch в виде двух функций: `BCELoss` и `BCEWithLogitsLoss`.\n",
    "\n",
    "Отличие данных функций состоит в том, что `BCELoss` вычисляет величину ошибки от выхода сети и требует, чтобы в архитектуре явно была указана функция активации на выходном слое. Функция `BCEWithLogitsLoss`, прежде чем вычислять величину потерь, применяет функцию активации (логистическую сигмоиду) к последнему слою, поэтому в архитектуре задавать функцию активации не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.FloatTensor(np.random.randint(0, 2, 5)) # float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Архитектура с `BCELoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 1),\n",
    "    torch.nn.Sigmoid() # явно задана функция активации\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X) # выдает вероятности\n",
    "    loss = criterion(pred, y.unsqueeze(1)) # размерность pred (5,1), размерность y - (5) \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred) # возвращает вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример с `BCEWithLogitsLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 1) # последний слой - линейный\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y.unsqueeze(1)) # размерности для BCEWithLogitsLoss должны совпадать\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred) # возвращает выход последнего лоинейного слоя\n",
    "print(torch.sigmoid(pred)) # преобразование в вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарная классификация &ndash; частный случай многоклассовой классификации. Для многоклассовой классификации используется кросс-энтропия. Функция потерь `CrossEntropyLoss` также не требует указания функции активации последним слоем в архитектуре сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.LongTensor(np.random.randint(0, 2, 5)) # CrossEntropyLoss требует формата int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 2) # на выходном слое 2 нейрона\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss() # применяет softmax к последнему слою и вычисляет ошибку\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.softmax(pred, 1)) # преобразование в вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Случай нескольких классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5, 3))\n",
    "y = torch.LongTensor(np.random.randint(0, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 10), \n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(10, 20), \n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 3) # 3 нейрона, соответствующие 3 классам\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, weight_decay=0.05)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = network(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.softmax(pred, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оформление архитектуры в виде класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_1 = torch.nn.Linear(3, 20)\n",
    "        self.layer_2 = torch.nn.Linear(20, 10)\n",
    "        self.layer_out = torch.nn.Linear(10, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output_1 = self.sigmoid(self.layer_1(inputs))\n",
    "        output_2 = self.sigmoid(self.layer_2(output_1))\n",
    "        output = self.layer_out(output_2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(np.random.randn(5,3))\n",
    "y = torch.FloatTensor(np.random.randint(0,2,5))\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X)\n",
    "    loss = criterion(pred, y.unsqueeze(1))\n",
    "    print('iter: ', i+1,' loss: ', loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h3> Упражнение</h3>\n",
    "<p></p>\n",
    "Реализовать и обучить модели из предыдущих заданий (binary_classification.csv, multiclass_classification.csv, светофор)\n",
    " <p></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
